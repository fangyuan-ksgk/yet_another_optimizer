{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer 1 has 1 parameter group(s)\n",
      "Optimizer 2 has 2 parameter group(s)\n",
      "Group 0 has lr=0.1 and contains 2 parameters\n",
      "Group 1 has lr=0.01 and contains 2 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Simple model with different parameter types\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Matrix parameters (2D)\n",
    "        self.weight1 = nn.Parameter(torch.randn(4, 8))\n",
    "        self.weight2 = nn.Parameter(torch.randn(8, 2))\n",
    "        # Vector parameters (1D)\n",
    "        self.bias1 = nn.Parameter(torch.randn(8))\n",
    "        self.bias2 = nn.Parameter(torch.randn(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x @ self.weight1 + self.bias1\n",
    "        x = x @ self.weight2 + self.bias2\n",
    "        return x\n",
    "\n",
    "def loss_fn(x, y): # MSE loss \n",
    "    return (x - y).norm()\n",
    "\n",
    "# Parameter group example\n",
    "model = ToyModel()\n",
    "\n",
    "input = torch.randn(4)\n",
    "output = model(input)\n",
    "loss = loss_fn(output, torch.randn(2))\n",
    "\n",
    "# Ok, so i can choose my own parameter groups when initializing the optimizer\n",
    "# - but once specified, parameter groups are fixed within the optimizer\n",
    "\n",
    "# Method 1: Using a single parameter group\n",
    "opt1 = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "print(f\"Optimizer 1 has {len(opt1.param_groups)} parameter group(s)\")\n",
    "\n",
    "# Method 2: Using multiple parameter groups with different learning rates\n",
    "opt2 = torch.optim.SGD([\n",
    "    {'params': [model.weight1, model.weight2], 'lr': 0.1},\n",
    "    {'params': [model.bias1, model.bias2], 'lr': 0.01}\n",
    "])\n",
    "print(f\"Optimizer 2 has {len(opt2.param_groups)} parameter group(s)\")\n",
    "\n",
    "# Print learning rates for each group\n",
    "for i, group in enumerate(opt2.param_groups):\n",
    "    print(f\"Group {i} has lr={group['lr']} and contains {len(group['params'])} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()\n",
    "# regarding\n",
    "\n",
    "def decide_rank(loss, max_loss=None, max_rank: int = 8, min_rank: int = 1):\n",
    "    # Use 'maximal loss' to decide rank value\n",
    "    if not max_loss: \n",
    "        max_loss = loss.item() \n",
    "    # ratio to decide rank value\n",
    "    return max(min_rank, int(loss.item() / max_loss * max_rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient \n",
    "for p in model.parameters(): \n",
    "    if p.ndim == 2: \n",
    "       # max_loss = self.state[p]['max_loss']\n",
    "       break \n",
    "\n",
    "p # tensor value \n",
    "p.grad # gradient value\n",
    "orig_grad = p.grad  \n",
    "r = decide_rank(loss, None, max_rank=min(p.shape))\n",
    "\n",
    "# SVD (2 rotation matrices, one vector of singular values)\n",
    "U, S, V = torch.linalg.svd(orig_grad, full_matrices=True)\n",
    "\n",
    "# Slicing important components from SVD results\n",
    "U[:,:r], S[:r], V[:r,:]\n",
    "\n",
    "# then accumulate 1st & 2nd moment of sliced values above\n",
    "# note - the 2nd moment should be 'row-wise norm' for V and 'column-wise norm' for U \n",
    "\n",
    "# are we missing the rotation matrix decomposition with magnitude vector, \n",
    "# - or do we have it already with SVD (U, V matrices) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 4]), torch.Size([8, 8]), torch.Size([4, 8]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.shape, V.shape, orig_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3848e+01, 6.6384e-07, 1.2254e-07, 8.4567e-09])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Optimizer assumes 'gradient' and 'parameter' are fixed\n",
    "# - Our idea combines 'wrapping low-rank adaptor' and call .backward() with optimization gadegts together \n",
    "# - in terms of code this is not just a custom optimizer, there needs to be another functionality happening before the .backward() functional .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

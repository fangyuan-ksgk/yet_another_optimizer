{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer 1 has 1 parameter group(s)\n",
      "Optimizer 2 has 2 parameter group(s)\n",
      "Group 0 has lr=0.1 and contains 2 parameters\n",
      "Group 1 has lr=0.01 and contains 2 parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Simple model with different parameter types\n",
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Matrix parameters (2D)\n",
    "        self.weight1 = nn.Parameter(torch.randn(4, 8))\n",
    "        self.weight2 = nn.Parameter(torch.randn(8, 2))\n",
    "        # Vector parameters (1D)\n",
    "        self.bias1 = nn.Parameter(torch.randn(8))\n",
    "        self.bias2 = nn.Parameter(torch.randn(2))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x @ self.weight1 + self.bias1\n",
    "        x = x @ self.weight2 + self.bias2\n",
    "        return x\n",
    "\n",
    "def loss_fn(x, y): # MSE loss \n",
    "    return (x - y).norm()\n",
    "\n",
    "# Parameter group example\n",
    "model = ToyModel()\n",
    "\n",
    "input = torch.randn(4)\n",
    "output = model(input)\n",
    "loss = loss_fn(output, torch.randn(2))\n",
    "\n",
    "# Ok, so i can choose my own parameter groups when initializing the optimizer\n",
    "# - but once specified, parameter groups are fixed within the optimizer\n",
    "\n",
    "# Method 1: Using a single parameter group\n",
    "opt1 = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "print(f\"Optimizer 1 has {len(opt1.param_groups)} parameter group(s)\")\n",
    "\n",
    "# Method 2: Using multiple parameter groups with different learning rates\n",
    "opt2 = torch.optim.SGD([\n",
    "    {'params': [model.weight1, model.weight2], 'lr': 0.1},\n",
    "    {'params': [model.bias1, model.bias2], 'lr': 0.01}\n",
    "])\n",
    "print(f\"Optimizer 2 has {len(opt2.param_groups)} parameter group(s)\")\n",
    "\n",
    "# Print learning rates for each group\n",
    "for i, group in enumerate(opt2.param_groups):\n",
    "    print(f\"Group {i} has lr={group['lr']} and contains {len(group['params'])} parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.9734, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss.backward()\n",
    "# regarding\n",
    "\n",
    "import torch \n",
    "import math \n",
    "\n",
    "def decide_rank(loss, max_loss=None, max_rank: int = 8, min_rank: int = 1):\n",
    "    # Use 'maximal loss' to decide rank value\n",
    "    if not max_loss: \n",
    "        max_loss = loss.item() \n",
    "    # ratio to decide rank value\n",
    "    return max(min_rank, int(loss.item() / max_loss * max_rank))\n",
    "\n",
    "\n",
    "@torch.compile # speed-up for overhead, might raise memory consumption\n",
    "def zeropower_via_newtonschulz5(G, steps):\n",
    "    \"\"\"\n",
    "    Newton-Schulz iteration to compute the zeroth power / orthogonalization of G. We opt to use a\n",
    "    quintic iteration whose coefficients are selected to maximize the slope at zero. For the purpose\n",
    "    of minimizing steps, it turns out to be empirically effective to keep increasing the slope at\n",
    "    zero even beyond the point where the iteration no longer converges all the way to one everywhere\n",
    "    on the interval. This iteration therefore does not produce UV^T but rather something like US'V^T\n",
    "    where S' is diagonal with S_{ii}' ~ Uniform(0.5, 1.5), which turns out not to hurt model\n",
    "    performance at all relative to UV^T, where USV^T = G is the SVD.\n",
    "    \"\"\"\n",
    "    assert len(G.shape) == 2\n",
    "    a, b, c = (3.4445, -4.7750, 2.0315)\n",
    "    X = G.bfloat16()\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    # Ensure spectral norm is at most 1\n",
    "    X = X / (X.norm() + 1e-7)\n",
    "    # Perform the NS iterations\n",
    "    for _ in range(steps):\n",
    "        A = X @ X.T\n",
    "        B = (\n",
    "            b * A + c * A @ A\n",
    "        )  # adapted from suggestion by @jxbz, @leloykun, and @YouJiacheng\n",
    "        X = a * X + B @ X\n",
    "\n",
    "    if G.size(0) > G.size(1):\n",
    "        X = X.T\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed grouping, no adjustment on magnitude & direction change version (ARG)\n",
    "\n",
    "# gradient \n",
    "for p in model.parameters(): \n",
    "    if p.ndim == 2: \n",
    "       # max_loss = self.state[p]['max_loss']\n",
    "       break \n",
    "\n",
    "p # tensor value \n",
    "p.grad # gradient value\n",
    "orig_grad = p.grad  \n",
    "r = decide_rank(loss, None, max_rank=min(p.shape))\n",
    "\n",
    "# SVD (2 rotation matrices, one vector of singular values)\n",
    "U, S, V = torch.svd_lowrank(orig_grad, q=r, niter=2)\n",
    "# lr_approx = U @ torch.diag(S) @ V.T # low rank approximation of gradient \n",
    "\n",
    "# momentum computations\n",
    "# For S, we accumulate 1st moment and 2nd moment like Adam \n",
    "# For U, V, we accumulate 1st moment and use NS to orthogonalize 1st momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 4]), torch.Size([4]), torch.Size([4, 4]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.shape, S.shape, U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.9877,  -4.5207,   9.5587,   5.0980],\n",
       "        [ -0.7154,  -1.1230,  -1.6472,   3.6788],\n",
       "        [  3.5238, -10.2778,  -0.1282,  -5.9239],\n",
       "        [  5.4887,   6.9303,   5.5392,  -0.3661],\n",
       "        [  1.0456,  -0.0203,  -0.3516,  -2.3887],\n",
       "        [  1.4112,  -2.2756,  -2.9140,   1.8091],\n",
       "        [ 10.7519,   1.5227,  -7.3522,   2.1512],\n",
       "        [ -2.2711,  -2.9870,  -2.0802,  10.1696]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S.norm() * V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional Optimizer assumes 'gradient' and 'parameter' are fixed\n",
    "# - Our idea combines 'wrapping low-rank adaptor' and call .backward() with optimization gadegts together \n",
    "# - in terms of code this is not just a custom optimizer, there needs to be another functionality happening before the .backward() functional .... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
